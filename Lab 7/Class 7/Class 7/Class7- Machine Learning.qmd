---
title: "Class 7: Machine Learning"
author: "Nhi To"
format: pdf
---

Today we wil explore unsupervised machine learning methods including clustering and dimensionality reduction methods. 

Let's start by making up some data (where we know there are clear gorups) that we can use to test out different clustering methods.

We can use the 'rnorm()' function to help us here:

```{r}
hist( rnorm(n=3000, mean=3) )
```

Make data with two "clusters'

```{r}
x <- c( rnorm(30, mean=-3), 
   rnorm(30, mean=+3) )

z <- cbind(x=x, rev(x))
head(z)

plot(z)

```

How big is 'z'
```{r}
nrow(z)
ncol(z)
```


## K-means clustering

The main function in "base" R for K-means clustering is called 'kmeans()'

```{r}

k <- kmeans(z, centers= 2)
k
```

```{r}
attributes(k)
```

> Q. How many points lie in each cluster?

```{r}
k$size
```

> Q. What component of our results tells us about the cluster membership (i.e. which pointlies in which cluster)?

```{r}
k$cluster
```

> Q, Center of each cluster?

```{r}
k$centers
```


>Q. Put this result info together and make a little "base R" plot of our clustering result. Also add the cluster center points to this plot. 

```{r}
plot (z, col= c("blue", "red"))

```


You can color by number 
```{r}
plot(z, col=c(1, 2))
```

Plot colored by cluster membership:

```{r}
plot(z, col= k$cluster)
points(k$centers, col="blue", pch=15)
```


> Q. Run kmeans on our input 'z' and define 4 clusters making the same result visualiation plot as plot as one (plot of z colored by cluster membership)

```{r}
k4 <- kmeans(z, centers=4)
plot(z, col= k4$cluster)

```


## Hiearchial CLustering

The main function in base R for this called 'hclus()' it will take as input a distance matrix(key point is that you can't just give your "raw" data as input- you have to first calculate a distance martrix from your data).

```{r}
d<- dist(z)
hc <- hclust(d)
hc
```

```{r}
plot(hc)
abline(h=10, col="red")
```

Once I inspect the "tree" I can "cut" the tree to yield  my groupings or clusters. The function to do this is called 'cutree()'

```{r}
grps<- cutree(hc, h=10)
```


```{r}
plot(z, col=grps)
```

## Hands on with Principal Component Analysis (PCA)

Let's examine some silly 17-dimensional data detailing food consumption in the UK (England, Scotland, Wales, and N. Ireland). Are these countries eating habits different or similr and if so how?


## Data import
```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url, row.names=1)
x
```

>Q1. How many rows and columns are in your new data frame named x? What R functions could you use to answer this questions?

>Answer: There are 17 rows and 4 columns


>Q2. Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?

>Answer:I approach "nrow()" and "ncol()" because it allows me to choose with one I would like to look at. If i use "dim()" I would be generating both. 

```{r}
nrow(x)
ncol(x)
dim(x)
```

>Q3. Changing what optional argument in the above barplot() function results in the following plot?

>Answer:Changing the argument from barplot(as.matrix(x), beside=T, col=rainbow(nrow(x))) to barplot(as.matrix(x), beside=F, col=rainbow(nrow(x))) would result in a stacked barplot instead of a grouped barplot. The argument being changed was beside=T to beside=F. 

```{r}
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))
```

>Q5: Generating all pairwise plots may help somewhat. Can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a given plot?

>Ans: if the points are diagonal on a given plot, it means that the values of the two compared countries are similar. If the point does not align diagonally, it means that they are not similar.

>Q6. What is the main differences between N. Ireland and the other countries of the UK in terms of this data-set?

>Ans: The main differences between N. Ireland the other countries of the UK are the consumption of potatoes, fresh fruit,  soft drinks, and alcoholic drinks. Ireland has consumed significantly more fresh potatoes than other countries of the UK. Ireland has consumed significantly less fresh fruit than the other countries of the UK. Ireland has consumed much more soft drinks than Wales. Ireland has consumed significantly less alcoholic dirnks than the other countries of the UK. 

```{r}
pairs(x, col=rainbow(nrow(x)), pch=16)
```

Looking at these types of "pairwise plots" can be helpful but it does not scale well and kind of sucks!There must be a better way...

## PCA to the rescue!

The main function for PCA is base R is called 'prcomp()'. This function wants the transpose of our input data- i.e. the important foods in as columns and the countries as rows. 


```{r}
pca <- prcomp(t(x))
summary(pca)
```

> Q7. Complete the code below to generate a plot of PC1 vs PC2. The second line adds text labels over the data points.

```{r}
# Plot PC1 vs PC2
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))
text(pca$x[,1], pca$x[,2], colnames(x))
```

>Q8. Customize your plot so that the colors of the country names match the colors in our UK and Ireland map and table at start of this document.

```{r}
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))
text(pca$x[,1], pca$x[,2], colnames(x), col=c("orange", "red", "blue", "darkgreen"))
```



Looking at these types of "pairwise plots" can be helpful but it does not scale well and kind of sucks!There must be a better way...


Let's see what is in our PCA result object 'pca'

```{r}
attributes(pca)
```

The 'pca$x' result object is where we will focus first as this details how the countries are related to each other in terms of our new "axis" (a.k.a "PCs", "eigenvectors", etc.)


```{r}
head(pca$x)
```

```{r}
plot(pca$x[,1], pca$x[,2], pch=16, col=c("orange", "red", "blue", "darkgreen"))
```

We can look at the so-called PC "loadings" result object to seehow the orignal foods contribute to our new  PCs (i.e. how the original variables contribute to our new better PC variables)

```{r}
pca$rotation[,1]
```


Calculating how much variation in the original data each PC accounts for

```{r}
v <- round( pca$sdev^2/sum(pca$sdev^2) * 100 )
v
```
```{r}
z <- summary(pca)
z$importance
```


Plot v
```{r}
barplot(v, xlab="Principal Component", ylab="Percent Variation")
```


```{r}
## Lets focus on PC1 as it accounts for > 90% of variance 
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,1], las=2 )
```


>Q9: Generate a similar ‘loadings plot’ for PC2. What two food groups feature prominantely and what does PC2 maninly tell us about?

>Answer: Fresh potatoes and soft drinks are the two food groups that are featured prominately in the 'loadings plot' for PC2. PC2 details the second-largest variance in the dataset, which means it captures variations not included in PC1.

```{r}
## Lets focus on PC2 
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,2], las=2 )
```

Using ggplot for the figures
```{r}
library(ggplot2)

df <- as.data.frame(pca$x)
df_lab <- tibble::rownames_to_column(df, "Country")

# Our first basic plot
ggplot(df_lab) + 
  aes(PC1, PC2, col=Country) + 
  geom_point()

```

```{r}
# a much nicer plot
ggplot(df_lab) + 
  aes(PC1, PC2, col=Country, label=Country) + 
  geom_hline(yintercept = 0, col="gray") +
  geom_vline(xintercept = 0, col="gray") +
  geom_point(show.legend = FALSE) +
  geom_label(hjust=1, nudge_x = -10, show.legend = FALSE) +
  expand_limits(x = c(-300,500)) +
  xlab("PC1 (67.4%)") +
  ylab("PC2 (28%)") +
  theme_bw()
```

```{r}
ld <- as.data.frame(pca$rotation)
ld_lab <- tibble::rownames_to_column(ld, "Food")

ggplot(ld_lab) +
  aes(PC1, Food) +
  geom_col() 
```
```{r}
## PCA of RNA-seq data
url2 <- "https://tinyurl.com/expression-CSV"
rna.data <- read.csv(url2, row.names=1)
head(rna.data)
```

>Q10: How many genes and samples are in this data set?

>Answer:There are 100 rows, which means there is 100 genes. There are 10 columns, which means there are 10 samples. 

```{r}
nrow(rna.data)
ncol(rna.data)
dim(rna.data)
```



